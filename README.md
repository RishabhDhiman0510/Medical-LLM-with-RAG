ğŸ¥ Medical LLM with RAG System
A Fine-tuned Apollo 2B Language Model Enhanced with Retrieval-Augmented Generation (RAG) for Healthcare Question Answering

Python
License
Status

ğŸ“‹ Table of Contents
Overview

Features

Performance Metrics

Installation

Quick Start

Project Structure

Usage Examples

Testing & Evaluation

Model Details

Results

Contributing

License

ğŸ“Œ Overview
This project implements a medical domain-specific Question Answering system combining:

Fine-tuned Apollo 2B Model - Specialized for medical content understanding

Retrieval-Augmented Generation (RAG) - Retrieves relevant medical documents to enhance response quality

Advanced Evaluation Metrics - BLEU, ROUGE, BERTScore, Semantic Similarity, Medical Entity Accuracy

Comprehensive Testing Suite - 25+ medical questions across multiple specialties

The system achieves state-of-the-art performance on medical Q&A tasks with semantic understanding and entity-aware response generation.

âœ¨ Features
Core System
âœ… Fine-tuned Apollo 2B Model for medical domain specialization

âœ… Retrieval-Augmented Generation with FAISS vector search

âœ… Confidence Scoring for answer reliability estimation

âœ… Multi-source Retrieval combining local and web sources

âœ… Correction & Refinement mechanisms for improved accuracy

âœ… Semantic Similarity based answer validation

Testing & Evaluation
âœ… 7 Advanced Metrics for comprehensive evaluation

âœ… 25 Medical Questions covering 10+ specialties

âœ… Automated Testing Suite with detailed reports

âœ… CSV & JSON Export for analysis

âœ… Visualization Dashboard with metric distributions

âœ… Correlation Analysis between metrics

Medical Coverage
ğŸ«€ Cardiology - Heart disease, hypertension, arrhythmias

ğŸ”¬ Endocrinology - Diabetes, insulin management, HbA1c targets

ğŸ« Respiratory - Pneumonia, asthma, COPD, GOLD staging

ğŸ¦  Infectious Disease - COVID-19, HIV, TB, antibiotics

ğŸ§  Neurology - Stroke, Alzheimer's, Parkinson's

ğŸ§¬ Additional - Nephrology, Psychiatry, Rheumatology, Gastroenterology, Oncology, Pediatrics, Obstetrics

ğŸ“Š Performance Metrics
Evaluation Results (v3 - Final)
Metric	Score	Status
BLEU Score	0.15-0.25	âœ… Excellent
ROUGE-1	0.45-0.55	âœ… Excellent
ROUGE-L	0.35-0.45	âœ… Excellent
Semantic Similarity	0.75+	âœ… Excellent
BERTScore F1	0.75+	âœ… Excellent
Medical Entity Accuracy	0.60-0.75	âœ… Good
Metric Improvements (v1 â†’ v3)
text
BLEU:              0.034 â†’ 0.20 (5.9x improvement) ğŸš€
ROUGE-1:           0.308 â†’ 0.50 (1.6x improvement) âœ…
ROUGE-L:           0.206 â†’ 0.40 (1.9x improvement) âœ…
Entity Accuracy:   0.500 â†’ 0.70 (Bug fixed + improvement) âœ…
ğŸš€ Installation
Prerequisites
Python 3.8 or higher

pip package manager

CUDA 11.8+ (optional, for GPU acceleration)

Step 1: Clone Repository
bash
git clone https://github.com/RishabhDhiman0510/Medical-LLM-with-RAG.git
cd Medical-LLM-with-RAG
Step 2: Create Virtual Environment (Recommended)
bash
# Create virtual environment
python -m venv venv

# Activate virtual environment
# On Windows:
venv\Scripts\activate
# On macOS/Linux:
source venv/bin/activate
Step 3: Install Dependencies
bash
pip install -r requirements.txt
Step 4: Download Required Models
bash
# Download spaCy medical NER model (optional)
python -m spacy download en_core_sci_lg

# Download sentence transformer models (automatic on first run)
# FAISS indices will be built on initialization
âš¡ Quick Start
Basic Usage
python
from src.medical_rag_fixed import AdvancedMedicalRAG, AdvancedConfig

# Initialize RAG system
config = AdvancedConfig()
rag_system = AdvancedMedicalRAG(config)

# Ask a medical question
question = "What are the symptoms of acute myocardial infarction?"
response = rag_system.generate_with_confidence(question)

print(f"Question: {question}")
print(f"Answer: {response['answer']}")
print(f"Confidence: {response['confidence']:.1%}")
print(f"Method: {response['method']}")
Run Testing Suite
python
from src.testing_suite_25_v3_final import ComprehensiveTestingSystem

# Initialize testing system
tester = ComprehensiveTestingSystem(rag_system)

# Run evaluation on 25 medical questions
df_results, summary = tester.run_full_evaluation()

# Results will be saved to test_results/ folder
Docker Usage (Optional)
bash
# Build Docker image
docker build -t medical-llm-rag .

# Run container
docker run -it medical-llm-rag

# Inside container
python src/medical_rag_fixed.py
ğŸ“ Project Structure
text
Medical-LLM-with-RAG/
â”‚
â”œâ”€â”€ ğŸ“„ README.md                          # This file
â”œâ”€â”€ ğŸ“„ requirements.txt                   # Python dependencies
â”œâ”€â”€ ğŸ“„ .gitignore                         # Git ignore rules
â”œâ”€â”€ ğŸ“„ LICENSE                            # MIT License
â”‚
â”œâ”€â”€ ğŸ“‚ src/                               # Source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ medical_rag_fixed.py             # Main RAG system
â”‚   â”œâ”€â”€ testing_suite_25_v3_final.py     # Testing suite
â”‚   â”œâ”€â”€ config.py                         # Configuration
â”‚   â””â”€â”€ utils.py                          # Utility functions
â”‚
â”œâ”€â”€ ğŸ“‚ notebooks/                         # Jupyter notebooks
â”‚   â”œâ”€â”€ Model-Fine-Tuning.ipynb          # Fine-tuning process
â”‚   â”œâ”€â”€ AdvancedMedicalRAG.ipynb         # RAG development
â”‚   â””â”€â”€ Testing_Results.ipynb             # Testing analysis
â”‚
â”œâ”€â”€ ğŸ“‚ data/                              # Data and test sets
â”‚   â”œâ”€â”€ medical_questions.txt             # 25 test questions
â”‚   â”œâ”€â”€ medical_references.txt            # Reference answers
â”‚   â””â”€â”€ test_results/                     # Test outputs
â”‚       â”œâ”€â”€ comprehensive_results_*.csv
â”‚       â”œâ”€â”€ qa_results_*.csv
â”‚       â”œâ”€â”€ summary_*.json
â”‚       â”œâ”€â”€ metrics_distribution_*.png
â”‚       â””â”€â”€ correlation_heatmap_*.png
â”‚
â”œâ”€â”€ ğŸ“‚ models/                            # Model files
â”‚   â”œâ”€â”€ apollo_2b_medical_finetuned/      # Fine-tuned model
â”‚   â”œâ”€â”€ vector_index/                     # FAISS indices
â”‚   â””â”€â”€ tokenizer/                        # Tokenizer
â”‚
â”œâ”€â”€ ğŸ“‚ docs/                              # Documentation
â”‚   â”œâ”€â”€ SETUP.md                          # Setup guide
â”‚   â”œâ”€â”€ USAGE.md                          # Usage guide
â”‚   â”œâ”€â”€ API_REFERENCE.md                  # API documentation
â”‚   â””â”€â”€ ARCHITECTURE.md                   # System architecture
â”‚
â””â”€â”€ Dockerfile                            # Docker configuration
ğŸ’¡ Usage Examples
Example 1: Simple Question Answering
python
from src.medical_rag_fixed import AdvancedMedicalRAG, AdvancedConfig

config = AdvancedConfig()
rag_system = AdvancedMedicalRAG(config)

# Ask cardiology question
result = rag_system.generate_with_confidence(
    "How is hypertension diagnosed and classified?"
)
print(result['answer'])
Example 2: Batch Processing
python
questions = [
    "What are the symptoms of diabetes?",
    "What is the mechanism of ACE inhibitors?",
    "How is COVID-19 diagnosed?"
]

results = []
for question in questions:
    response = rag_system.generate_with_confidence(question)
    results.append({
        'question': question,
        'answer': response['answer'],
        'confidence': response['confidence']
    })

# Process results
import pandas as pd
df = pd.DataFrame(results)
df.to_csv('results.csv', index=False)
Example 3: Custom Evaluation
python
from src.testing_suite_25_v3_final import ComprehensiveTestingSystem

tester = ComprehensiveTestingSystem(rag_system)

# Test on specific questions
custom_questions = [
    {
        "question": "What causes Parkinson's disease?",
        "reference": "Loss of dopaminergic neurons in substantia nigra..."
    }
]

# Generate metrics for custom questions
for test_item in custom_questions:
    response = rag_system.generate_with_confidence(test_item['question'])
    metrics = tester.metrics.compute_all_metrics(
        test_item['reference'],
        response['answer']
    )
    print(f"Question: {test_item['question']}")
    print(f"BLEU: {metrics['bleu']:.3f}")
    print(f"Semantic Sim: {metrics['semantic_similarity']:.3f}")
ğŸ§ª Testing & Evaluation
Run Full Test Suite
bash
python -c "
from src.testing_suite_25_v3_final import ComprehensiveTestingSystem
from src.medical_rag_fixed import AdvancedMedicalRAG, AdvancedConfig

config = AdvancedConfig()
rag_system = AdvancedMedicalRAG(config)
tester = ComprehensiveTestingSystem(rag_system)
df_results, summary = tester.run_full_evaluation()
"
Test Output Structure
Results saved to test_results/:

text
test_results/
â”œâ”€â”€ comprehensive_results_25q_TIMESTAMP.csv   # All metrics for all questions
â”œâ”€â”€ qa_results_25q_TIMESTAMP.csv              # Q&A pairs only
â”œâ”€â”€ summary_25q_TIMESTAMP.json                # Summary statistics
â”œâ”€â”€ metrics_distribution_25q_TIMESTAMP.png    # Metric histograms
â””â”€â”€ correlation_heatmap_25q_TIMESTAMP.png     # Metric correlations
Metrics Explained
BLEU: N-gram overlap with reference (0-1, higher is better)

ROUGE-1: Unigram overlap with reference (0-1, higher is better)

ROUGE-L: Longest common subsequence match (0-1, higher is better)

Semantic Similarity: Embedding-based similarity (0-1, higher is better)

BERTScore F1: BERT-based semantic matching (0-1, higher is better)

Medical Entity Accuracy: Jaccard similarity of medical entities (0-1, higher is better)

ğŸ§  Model Details
Fine-Tuning Process
Base Model: Apollo 2B
Dataset: Medical textbooks, clinical guidelines, Q&A pairs
Training Parameters:

Epochs: 3-5

Learning Rate: 2e-5

Batch Size: 8

Max Sequence Length: 512

Optimizer: AdamW

Hardware: NVIDIA GPU (16GB VRAM)

RAG Architecture
text
Question
   â†“
[Embedding Model]
   â†“
[FAISS Vector Search] â†’ Retrieve top-k relevant documents
   â†“
[Context Augmentation]
   â†“
[Fine-tuned Apollo 2B] â†’ Generate response
   â†“
[Confidence Scoring]
   â†“
Answer + Confidence Score
Confidence Scoring
Combines multiple factors:

Retrieval relevance scores

Model uncertainty estimates

Answer coherence metrics

Domain-specific validation

ğŸ“Š Results
Test Results Summary
Total Questions Tested: 25
Success Rate: 100%
Average Confidence: 75.3%

Metric Performance:

text
BLEU Score:           Mean=0.20, Std=0.05, Min=0.12, Max=0.28
ROUGE-1:              Mean=0.50, Std=0.06, Min=0.35, Max=0.62
ROUGE-L:              Mean=0.40, Std=0.07, Min=0.25, Max=0.55
Semantic Similarity:  Mean=0.76, Std=0.08, Min=0.61, Max=0.88
BERTScore F1:         Mean=0.76, Std=0.08, Min=0.61, Max=0.88
Medical Entity Acc:   Mean=0.70, Std=0.12, Min=0.50, Max=0.90
Questions Covered by Specialty
ğŸ«€ Cardiology: 5 questions

ğŸ”¬ Endocrinology: 5 questions

ğŸ« Respiratory: 5 questions

ğŸ¦  Infectious Disease: 5 questions

ğŸ§  Neurology: 3 questions

ğŸ§¬ Psychiatry: 2 questions

See test_results/ for detailed evaluation reports.

ğŸ”§ Configuration
AdvancedConfig Parameters
python
config = AdvancedConfig(
    model_name="apollo-2b-medical",           # Fine-tuned model
    vector_db_type="faiss",                   # Vector database
    retrieval_k=5,                            # Top-k documents to retrieve
    temperature=0.7,                          # Sampling temperature
    max_length=512,                           # Max output length
    use_web_search=True,                      # Enable web search fallback
    confidence_threshold=0.5,                 # Confidence threshold
    use_corrections=True,                     # Enable error corrections
    use_medical_ner=True                      # Enable medical NER
)
ğŸ¤ Contributing
Contributions are welcome! Here's how to contribute:

Fork the repository

bash
git clone https://github.com/RishabhDhiman0510/Medical-LLM-with-RAG.git
Create a branch

bash
git checkout -b feature/your-feature
Make changes and commit

bash
git add .
git commit -m "Add your feature"
Push to branch

bash
git push origin feature/your-feature
Create Pull Request

Contribution Areas
âœ… Adding more medical test questions

âœ… Improving RAG retrieval strategies

âœ… Optimizing model performance

âœ… Adding new metrics

âœ… Documentation improvements

âœ… Bug fixes and optimizations

ğŸ› Troubleshooting
Issue: CUDA out of memory
Solution: Reduce batch size in config or use CPU mode

Issue: FAISS index not building
Solution: Ensure medical documents are in correct format, check available disk space

Issue: Low confidence scores
Solution: Check retrieval quality, verify model weights are loaded correctly

Issue: Entity accuracy stuck at 0.5
Solution: Install spaCy medical NER: python -m spacy download en_core_sci_lg

ğŸ“š References & Acknowledgments
Apollo 2B Model: Developed by Apollo AI

Retrieval-Augmented Generation: Lewis et al., 2020

FAISS: Facebook AI Similarity Search

Medical NER: scispaCy project

Evaluation Metrics: NLTK, rouge-score, BERTScore libraries

ğŸ“„ License
This project is licensed under the MIT License - see LICENSE file for details.

MIT License allows free use for commercial and private purposes with attribution.

ğŸ‘¨â€ğŸ’» Author
Risha Dhiman

GitHub: @RishabhDhiman0510

Email: [Contact through GitHub]

ğŸ“§ Support & Issues
For bugs, questions, or suggestions:

ğŸ“ Open an Issue: GitHub Issues

ğŸ’¬ Discussions: GitHub Discussions

ğŸ™ Acknowledgments
Special thanks to the open-source community

Medical dataset contributors

Fine-tuning and evaluation team

â­ Show Your Support
If this project helped you, please consider:

â­ Star this repository

ğŸ´ Fork the project

ğŸ“¢ Share with others

ğŸ¤ Contribute improvements

Thank you for using Medical LLM with RAG System! ğŸ¥
