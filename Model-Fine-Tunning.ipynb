{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f7f7dd8-ccc2-478f-bd67-db477c111abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ APOLLO-2B MEDICAL FINE-TUNING PIPELINE\n",
      "==================================================\n",
      "PyTorch version: 2.5.0+cu118\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 4070\n",
      "GPU Memory: 12.9 GB\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 1: Environment Setup and Dependencies\n",
    "# Install required packages and set up environment variables for memory optimization\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "import gc\n",
    "\n",
    "# Memory optimization environment variables\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "print(\"üöÄ APOLLO-2B MEDICAL FINE-TUNING PIPELINE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd53dc2a-10a9-493c-80c5-c001d3f2c560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loading Medical Q&A Dataset...\n",
      "‚úÖ Loaded 25000 Q&A records\n",
      "\n",
      "üîç DATASET VALIDATION:\n",
      "==============================\n",
      "‚úÖ question: Present\n",
      "‚úÖ answer: Present\n",
      "‚úÖ training_text: Present\n",
      "\n",
      "üìù SAMPLE Q&A PAIR:\n",
      "Question: What is A simplified scleral reinforcement technique. and what should I know about it?...\n",
      "Answer: Explanation: A simplified scleral reinforcement technique. A simplified scleral reinforcement technique performed on 52 eyes with myopic degeneration prevented further visual loss by strengthening of ...\n",
      "Training format length: 928 characters\n",
      "\n",
      "üìà DATASET STATISTICS:\n",
      "Average question length: 128 characters\n",
      "Average answer length: 1095 characters\n",
      "Total training samples: 25000\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: Load and Validate Processed Medical Q&A Dataset\n",
    "# Load the structured medical Q&A data and validate format\n",
    "# =============================================================================\n",
    "\n",
    "def load_medical_qa_dataset(file_path=\"medical_qa_training_data.jsonl\", max_samples=10000):\n",
    "    \"\"\"Load and validate medical Q&A dataset\"\"\"\n",
    "    \n",
    "    print(\"üìä Loading Medical Q&A Dataset...\")\n",
    "    \n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= max_samples:  # Limit for 6GB VRAM\n",
    "                break\n",
    "            record = json.loads(line.strip())\n",
    "            data.append(record)\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(data)} Q&A records\")\n",
    "    return data\n",
    "\n",
    "# Load dataset\n",
    "qa_data = load_medical_qa_dataset(max_samples=25000)  # Adjust based on your VRAM\n",
    "\n",
    "# Validation: Check dataset structure\n",
    "print(\"\\nüîç DATASET VALIDATION:\")\n",
    "print(\"=\"*30)\n",
    "sample = qa_data[0]\n",
    "required_fields = ['question', 'answer', 'training_text']\n",
    "for field in required_fields:\n",
    "    status = \"‚úÖ\" if field in sample else \"‚ùå\"\n",
    "    print(f\"{status} {field}: {'Present' if field in sample else 'Missing'}\")\n",
    "\n",
    "print(f\"\\nüìù SAMPLE Q&A PAIR:\")\n",
    "print(f\"Question: {sample['question'][:100]}...\")\n",
    "print(f\"Answer: {sample['answer'][:200]}...\")\n",
    "print(f\"Training format length: {len(sample['training_text'])} characters\")\n",
    "\n",
    "print(f\"\\nüìà DATASET STATISTICS:\")\n",
    "avg_question_len = sum(len(item['question']) for item in qa_data) / len(qa_data)\n",
    "avg_answer_len = sum(len(item['answer']) for item in qa_data) / len(qa_data)\n",
    "print(f\"Average question length: {avg_question_len:.0f} characters\")\n",
    "print(f\"Average answer length: {avg_answer_len:.0f} characters\")\n",
    "print(f\"Total training samples: {len(qa_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad50ff3b-6186-4477-9571-ebaeb4303bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Preparing Training Dataset...\n",
      "‚úÖ Created training dataset with 25000 samples\n",
      "\n",
      "üîç TRAINING DATASET VALIDATION:\n",
      "===================================\n",
      "Dataset size: 25000\n",
      "Dataset features: {'text': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None), 'answer': Value(dtype='string', id=None)}\n",
      "\n",
      "üìù SAMPLE TRAINING TEXT:\n",
      "Question: What is A simplified scleral reinforcement technique. and what should I know about it?\n",
      "Answer: Explanation: A simplified scleral reinforcement technique. A simplified scleral reinforcement technique performed on 52 eyes with myopic degeneration prevented further visual loss by strengthenin...\n",
      "\n",
      "üìä TEXT LENGTH STATISTICS:\n",
      "Min length: 516\n",
      "Max length: 3599\n",
      "Average length: 1242\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 3: Convert to HuggingFace Dataset Format\n",
    "# Prepare the Q&A data in the format required for SFTTrainer\n",
    "# =============================================================================\n",
    "\n",
    "def prepare_training_dataset(qa_data):\n",
    "    \"\"\"Convert Q&A data to HuggingFace Dataset format\"\"\"\n",
    "    \n",
    "    print(\"üîÑ Preparing Training Dataset...\")\n",
    "    \n",
    "    # Extract training texts for the model\n",
    "    training_data = []\n",
    "    for item in qa_data:\n",
    "        training_data.append({\n",
    "            \"text\": item[\"training_text\"],\n",
    "            \"question\": item[\"question\"],\n",
    "            \"answer\": item[\"answer\"]\n",
    "        })\n",
    "    \n",
    "    # Create HuggingFace Dataset\n",
    "    dataset = Dataset.from_list(training_data)\n",
    "    \n",
    "    print(f\"‚úÖ Created training dataset with {len(dataset)} samples\")\n",
    "    return dataset\n",
    "\n",
    "# Prepare dataset\n",
    "train_dataset = prepare_training_dataset(qa_data)\n",
    "\n",
    "# Validation: Check dataset format\n",
    "print(\"\\nüîç TRAINING DATASET VALIDATION:\")\n",
    "print(\"=\"*35)\n",
    "print(f\"Dataset size: {len(train_dataset)}\")\n",
    "print(f\"Dataset features: {train_dataset.features}\")\n",
    "\n",
    "# Show sample training text\n",
    "sample_text = train_dataset[0]['text']\n",
    "print(f\"\\nüìù SAMPLE TRAINING TEXT:\")\n",
    "print(sample_text[:300] + \"...\" if len(sample_text) > 300 else sample_text)\n",
    "\n",
    "# Check text lengths for sequence optimization\n",
    "text_lengths = [len(item['text']) for item in train_dataset]\n",
    "print(f\"\\nüìä TEXT LENGTH STATISTICS:\")\n",
    "print(f\"Min length: {min(text_lengths)}\")\n",
    "print(f\"Max length: {max(text_lengths)}\")\n",
    "print(f\"Average length: {sum(text_lengths)/len(text_lengths):.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f99519d1-58f3-4483-9970-eca821e4aace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Setting up Apollo-2B Model...\n",
      "Loading model from HuggingFace...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4dbdce9dee146c4ba2d226af9e16fda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a323155a3d44c2ab2dd8d5e21564667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06111b3e57c345f69262e2f8861e90bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d88948dae0a4cafbfbfdc808fe65d46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1a4d4baa6e34d4fb84559c6ba153413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/134M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce39e0152a2a4f84a1e4292f23bbfc3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/FreedomIntelligence/Apollo-2B/resolve/4049fc07e95649c91e86be4458faf935bdc106b8/model-00002-of-00003.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Error while downloading from https://huggingface.co/FreedomIntelligence/Apollo-2B/resolve/4049fc07e95649c91e86be4458faf935bdc106b8/model-00001-of-00003.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "960af3d915a2469dbc8dddc29e63790e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   4%|3         | 189M/4.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d14b345b3fa4346a8f866c5a7553b99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   5%|4         | 231M/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb56b30b223646b48a328b3b3731fc97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d91dd1521dc43378d53b7ad4bf95c4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14923d594e1148d1a36dc4b15fb4009e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c7f671f73e47f4bb8719dd157678c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6b5df9e57c34748a10af08958e8e28d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9146370855c649f6a1a7eff74c2c0548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/555 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model and tokenizer loaded successfully\n",
      "\n",
      "üîç MODEL VALIDATION:\n",
      "=========================\n",
      "Model type: GemmaForCausalLM\n",
      "Model device: cuda:0\n",
      "Model dtype: torch.float16\n",
      "Tokenizer vocab size: 256000\n",
      "Pad token: <eos>\n",
      "GPU memory allocated: 2.07 GB\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 4: Load Apollo-2B Model with 4-bit Quantization\n",
    "# Setup the model for efficient training on 6GB VRAM\n",
    "# =============================================================================\n",
    "\n",
    "def setup_apollo_model():\n",
    "    \"\"\"Load Apollo-2B with QLoRA configuration for 6GB VRAM\"\"\"\n",
    "    \n",
    "    print(\"ü§ñ Setting up Apollo-2B Model...\")\n",
    "    \n",
    "    # Quantization config for memory efficiency\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_quant_storage=torch.uint8\n",
    "    )\n",
    "    \n",
    "    print(\"Loading model from HuggingFace...\")\n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"FreedomIntelligence/Apollo-2B\",\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"FreedomIntelligence/Apollo-2B\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Setup tokenizer\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    print(\"‚úÖ Model and tokenizer loaded successfully\")\n",
    "    return model, tokenizer\n",
    "\n",
    "# Load model and tokenizer\n",
    "model, tokenizer = setup_apollo_model()\n",
    "\n",
    "# Validation: Check model loading\n",
    "print(\"\\nüîç MODEL VALIDATION:\")\n",
    "print(\"=\"*25)\n",
    "print(f\"Model type: {type(model).__name__}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "print(f\"Model dtype: {next(model.parameters()).dtype}\")\n",
    "print(f\"Tokenizer vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"Pad token: {tokenizer.pad_token}\")\n",
    "\n",
    "# Check GPU memory usage\n",
    "if torch.cuda.is_available():\n",
    "    memory_allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    print(f\"GPU memory allocated: {memory_allocated:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b600bc07-0066-4307-ba7a-55463a972a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Setting up LoRA Configuration...\n",
      "‚úÖ LoRA configuration applied\n",
      "\n",
      "üîç LORA VALIDATION:\n",
      "======================\n",
      "Total parameters: 2,525,784,064\n",
      "Trainable parameters: 19,611,648\n",
      "Trainable percentage: 0.78%\n",
      "Memory reduction: ~99.2%\n",
      "trainable params: 19,611,648 || all params: 2,525,784,064 || trainable%: 0.7765\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 5: Setup LoRA Configuration\n",
    "# Configure parameter-efficient fine-tuning to reduce memory usage\n",
    "# =============================================================================\n",
    "\n",
    "def setup_lora(model):\n",
    "    \"\"\"Configure LoRA for parameter-efficient fine-tuning\"\"\"\n",
    "    \n",
    "    print(\"üîß Setting up LoRA Configuration...\")\n",
    "    \n",
    "    # Prepare model for k-bit training\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    \n",
    "    # LoRA configuration optimized for 6GB VRAM\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,                    # Rank - balance between performance and memory\n",
    "        lora_alpha=32,          # Alpha parameter\n",
    "        target_modules=[        # Target modules for LoRA\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "        ],\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    \n",
    "    # Apply LoRA to model\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    print(\"‚úÖ LoRA configuration applied\")\n",
    "    return model\n",
    "\n",
    "# Apply LoRA configuration\n",
    "model = setup_lora(model)\n",
    "\n",
    "# Validation: Check LoRA parameters\n",
    "print(\"\\nüîç LORA VALIDATION:\")\n",
    "print(\"=\"*22)\n",
    "total_params = model.num_parameters()\n",
    "trainable_params = model.num_parameters(only_trainable=True)\n",
    "trainable_percentage = (trainable_params / total_params) * 100\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Trainable percentage: {trainable_percentage:.2f}%\")\n",
    "print(f\"Memory reduction: ~{100 - trainable_percentage:.1f}%\")\n",
    "\n",
    "# Print trainable modules\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30f6b04a-9fda-4d18-a2c8-69004fbebb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Configuring Training Parameters...\n",
      "‚úÖ Training arguments configured\n",
      "\n",
      "üîç TRAINING CONFIGURATION:\n",
      "==============================\n",
      "Batch size: 2\n",
      "Gradient accumulation: 8\n",
      "Effective batch size: 16\n",
      "Learning rate: 0.0001\n",
      "Number of epochs: 2\n",
      "Total training steps: 3124\n",
      "Optimizer: OptimizerNames.PAGED_ADAMW_8BIT\n",
      "Mixed precision: True\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 6: Training Configuration\n",
    "# Setup training parameters optimized for 6GB VRAM and medical data\n",
    "# =============================================================================\n",
    "\n",
    "def setup_training_args(output_dir=\"./apollo2b-medical-qa\"):\n",
    "    \"\"\"Configure training arguments for medical fine-tuning\"\"\"\n",
    "    \n",
    "    print(\"‚öôÔ∏è Configuring Training Parameters...\")\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        \n",
    "        # Memory optimization for 6GB VRAM\n",
    "        per_device_train_batch_size=2,      # Small batch size\n",
    "        gradient_accumulation_steps=8,       # Effective batch size = 8\n",
    "        gradient_checkpointing=True,         # Trade compute for memory\n",
    "        dataloader_pin_memory=False,         # Reduce memory transfer\n",
    "        \n",
    "        # Training parameters\n",
    "        num_train_epochs=2,                  # Number of epochs\n",
    "        learning_rate=1e-4,                  # Learning rate\n",
    "        warmup_steps=100,                    # Warmup steps\n",
    "        logging_steps=25,                    # Log every 25 steps\n",
    "        save_steps=500,                      # Save every 500 steps\n",
    "        eval_steps=500,                      # Evaluation frequency\n",
    "        \n",
    "        # Optimizer settings\n",
    "        optim=\"paged_adamw_8bit\",           # Memory-efficient optimizer\n",
    "        lr_scheduler_type=\"cosine\",         # Learning rate scheduler\n",
    "        weight_decay=0.01,                  # Weight decay\n",
    "        max_grad_norm=1.0,                  # Gradient clipping\n",
    "        \n",
    "        # Precision and efficiency\n",
    "        fp16=True,                          # Mixed precision training\n",
    "        remove_unused_columns=False,\n",
    "        \n",
    "        # Monitoring and saving\n",
    "        report_to=None,                     # Disable wandb for now\n",
    "        save_total_limit=3,                 # Keep only 3 checkpoints\n",
    "        load_best_model_at_end=False,\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Training arguments configured\")\n",
    "    return training_args\n",
    "\n",
    "# Setup training arguments\n",
    "training_args = setup_training_args()\n",
    "\n",
    "# Validation: Check training configuration\n",
    "print(\"\\nüîç TRAINING CONFIGURATION:\")\n",
    "print(\"=\"*30)\n",
    "print(f\"Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"Number of epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"Total training steps: {(len(train_dataset) // training_args.per_device_train_batch_size // training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")\n",
    "print(f\"Optimizer: {training_args.optim}\")\n",
    "print(f\"Mixed precision: {training_args.fp16}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48390a47-0b67-4986-b781-c7d3ddea2569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèãÔ∏è Initializing Trainer with labels for causal LM...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b93db7b245a4606b900ee151bdb6221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jainb\\AppData\\Local\\Temp\\ipykernel_3604\\2197264891.py:34: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 1}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenized dataset with columns: ['input_ids', 'attention_mask', 'labels']\n",
      "‚úÖ Trainer initialized\n",
      "GPU memory before training: 3.20 GB\n",
      "\n",
      "üöÄ Starting Medical Fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3126' max='3126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3126/3126 5:06:48, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.922100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.129600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.005300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.933700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.946100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.909400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.904600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.911300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.899100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.915600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.914600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.942400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.926200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.909000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.915700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.894500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.919500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.906600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.893100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.923700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.918700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>0.900600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.896700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.894800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.935800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.873000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.959600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.908900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.872500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>0.881500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.879600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>0.869100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.893500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>0.899300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.881500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>0.917800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.911200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>0.913500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.906500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>0.885000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.936700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1075</td>\n",
       "      <td>0.901100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.903200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1125</td>\n",
       "      <td>0.903200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.890700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1175</td>\n",
       "      <td>0.925000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.886600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1225</td>\n",
       "      <td>0.917000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.886300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1275</td>\n",
       "      <td>0.871400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.957000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1325</td>\n",
       "      <td>0.894600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.896300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1375</td>\n",
       "      <td>0.890800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.914300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1425</td>\n",
       "      <td>0.896300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.924000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1475</td>\n",
       "      <td>0.920400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.881800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1525</td>\n",
       "      <td>0.892700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.909500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1575</td>\n",
       "      <td>0.874000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.873000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1625</td>\n",
       "      <td>0.860700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.868500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1675</td>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.879400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1725</td>\n",
       "      <td>0.890300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.871500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1775</td>\n",
       "      <td>0.827700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.883200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1825</td>\n",
       "      <td>0.872800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.876400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1875</td>\n",
       "      <td>0.865400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.868200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1925</td>\n",
       "      <td>0.876100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.883500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1975</td>\n",
       "      <td>0.869400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.898300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025</td>\n",
       "      <td>0.866000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.859300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2075</td>\n",
       "      <td>0.819500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.861500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2125</td>\n",
       "      <td>0.858000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.878400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2175</td>\n",
       "      <td>0.880700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.891300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2225</td>\n",
       "      <td>0.903700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.863700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2275</td>\n",
       "      <td>0.864600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.881800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2325</td>\n",
       "      <td>0.856600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.885700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2375</td>\n",
       "      <td>0.876000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.863700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2425</td>\n",
       "      <td>0.866800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.877800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2475</td>\n",
       "      <td>0.868900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.824200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2525</td>\n",
       "      <td>0.874400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.860100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2575</td>\n",
       "      <td>0.835000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.912700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2625</td>\n",
       "      <td>0.880400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.835900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2675</td>\n",
       "      <td>0.876500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.857500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2725</td>\n",
       "      <td>0.843300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.874200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2775</td>\n",
       "      <td>0.875800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.865800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2825</td>\n",
       "      <td>0.851800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.860200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2875</td>\n",
       "      <td>0.830200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.871200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2925</td>\n",
       "      <td>0.848300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.833200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2975</td>\n",
       "      <td>0.860200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.866100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3025</td>\n",
       "      <td>0.864700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.859200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3075</td>\n",
       "      <td>0.892100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.885100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3125</td>\n",
       "      <td>0.855500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training completed!\n",
      "GPU memory after training: 3.23 GB\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 7 (Fix): Initialize transformers.Trainer with labels for causal LM\n",
    "# =============================================================================\n",
    "\n",
    "from transformers import Trainer, default_data_collator\n",
    "\n",
    "def tokenize_and_add_labels(examples):\n",
    "    \"\"\"Tokenize and add labels for causal LM\"\"\"\n",
    "    # Tokenize text\n",
    "    outputs = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"longest\",\n",
    "        max_length=512\n",
    "    )\n",
    "    # For causal LM, labels are identical to input_ids\n",
    "    outputs[\"labels\"] = outputs[\"input_ids\"].copy()\n",
    "    return outputs\n",
    "\n",
    "def start_medical_training_v2(model, tokenizer, train_dataset, training_args):\n",
    "    \"\"\"Initialize Trainer for causal LM and start fine-tuning\"\"\"\n",
    "    \n",
    "    print(\"üèãÔ∏è Initializing Trainer with labels for causal LM...\")\n",
    "    \n",
    "    # Tokenize and add labels\n",
    "    tokenized_ds = train_dataset.map(\n",
    "        tokenize_and_add_labels,\n",
    "        batched=True,\n",
    "        remove_columns=[\"text\", \"question\", \"answer\"]\n",
    "    )\n",
    "    print(f\"‚úÖ Tokenized dataset with columns: {tokenized_ds.column_names}\")\n",
    "    \n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_ds,\n",
    "        data_collator=default_data_collator,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Trainer initialized\")\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"GPU memory before training: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    \n",
    "    print(\"\\nüöÄ Starting Medical Fine-tuning...\")\n",
    "    trainer.train()\n",
    "    print(\"‚úÖ Training completed!\")\n",
    "    \n",
    "    return trainer\n",
    "\n",
    "# Start training\n",
    "trainer = start_medical_training_v2(model, tokenizer, train_dataset, training_args)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU memory after training: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f66fff72-e52a-4565-915b-0abf53942e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving Fine-tuned Medical Model...\n",
      "‚úÖ Model saved to: ./apollo2b-medical-qa-final\n",
      "\n",
      "üìÅ Saved files: ['adapter_config.json', 'adapter_model.safetensors', 'README.md', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer.model', 'tokenizer_config.json', 'training_args.bin']\n",
      "\n",
      "üéâ MEDICAL MODEL TRAINING COMPLETE!\n",
      "========================================\n",
      "‚úÖ Model successfully fine-tuned on 25000 medical Q&A pairs\n",
      "‚úÖ Model saved to: ./apollo2b-medical-qa-final\n",
      "‚úÖ Ready for medical question answering!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 8: Save Fine-tuned Medical Model\n",
    "# Save the trained model and tokenizer for future use\n",
    "# =============================================================================\n",
    "\n",
    "def save_medical_model(trainer, tokenizer, save_path=\"./apollo2b-medical-qa-final\"):\n",
    "    \"\"\"Save the fine-tuned medical model\"\"\"\n",
    "    \n",
    "    print(\"üíæ Saving Fine-tuned Medical Model...\")\n",
    "    \n",
    "    # Save model\n",
    "    trainer.save_model(save_path)\n",
    "    \n",
    "    # Save tokenizer\n",
    "    tokenizer.save_pretrained(save_path)\n",
    "    \n",
    "    print(f\"‚úÖ Model saved to: {save_path}\")\n",
    "    \n",
    "    # Validation: Check saved files\n",
    "    import os\n",
    "    saved_files = os.listdir(save_path)\n",
    "    print(f\"\\nüìÅ Saved files: {saved_files}\")\n",
    "    \n",
    "    return save_path\n",
    "\n",
    "# Save the model\n",
    "model_path = save_medical_model(trainer, tokenizer)\n",
    "\n",
    "print(\"\\nüéâ MEDICAL MODEL TRAINING COMPLETE!\")\n",
    "print(\"=\"*40)\n",
    "print(f\"‚úÖ Model successfully fine-tuned on {len(train_dataset)} medical Q&A pairs\")\n",
    "print(f\"‚úÖ Model saved to: {model_path}\")\n",
    "print(f\"‚úÖ Ready for medical question answering!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
